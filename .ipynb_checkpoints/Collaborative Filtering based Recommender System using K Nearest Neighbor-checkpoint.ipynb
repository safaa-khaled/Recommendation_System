{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML321ENSkillsNetwork817-2022-01-01\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Collaborative Filtering based Recommender System using K Nearest Neighbor**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **60** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collaborative filtering is probably the most commonly used recommendation algorithm, there are two main types of methods: \n",
    " - **User-based** collaborative filtering is based on the user similarity or neighborhood\n",
    " - **Item-based** collaborative filtering is based on similarity among items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They both work similarly, let's briefly explain how user-based collaborative filtering works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User-based collaborative filtering looks for users who are similar. This is very similar to the user clustering method done previously; where we employed explicit user profiles to calculate user similarity. However, the user profiles may not be available, so how can we determine if two users are similar?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User-item interaction matrix \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most collaborative filtering-based recommender systems, the main dataset format is a 2-D matrix called the user-item interaction matrix. In the matrix,  its row is labeled as the user id/index and column labelled to be the item id/index, and the element `(i, j)` represents the rating of user `i` to item `j`.  \n",
    "\n",
    "Below is a simple example of a user-item interaction matrix:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML321EN-SkillsNetwork/labs/module_4/images/user_item_matrix.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN-based collaborative filtering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from above, each row vector represents the rating history of a user and each column vector represents the users who rated the item. A user-item interaction matrix is usually very sparse as you can imagine one user very likely only interacts with a very small subset of items and one item is very likely to be interacted by a small subset of users.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to determine if two users are similar, we can simply calculate the similarities between their row vectors in the interaction matrix. Then based on the similarity measurements, we can find the `k` nearest neighbor as the similar users.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Item-based collaborative filtering works similarly, we just need to look at the user-item matrix vertically. Instead of finding similar users, we are trying to find similar items (courses). If two courses are enrolled by two groups of similar users, then we could consider the two items are similar and use the known ratings from the other users to predict the unknown ratings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we formulate the KNN based collaborative filtering,  the predicted rating of user $u$ to item $i$, $\\hat{r}_{ui}$ is given by:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User-based** collaborative filtering:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{r}_{ui} = \\frac{\n",
    "\\sum\\limits_{v \\in N^k_i(u)} \\text{similarity}(u, v) \\cdot r_{vi}}\n",
    "{\\sum\\limits_{v \\in N^k_i(u)} \\text{similarity}(u, v)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Item-based** collaborative filtering:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{r}_{ui} = \\frac{\n",
    "\\sum\\limits_{j \\in N^k_u(i)} \\text{similarity}(i, j) \\cdot r_{uj}}\n",
    "{\\sum\\limits_{j \\in N^k_u(i)} \\text{similarity}(i, j)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $N^k_i(u)$ notates the nearest k neighbors of $u$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's illustrate how the equation works using a simple example. From the above figure, suppose we want to predict the rating of `user6` to item `Machine Learning Capstone` course. After some similarity measurements, we found that k = 4 nearest neighbors: `user2, user3, user4, user5` with similarities in array ```knn_sims```:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from scipy.spatial.distance import cosine,euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example similarity array stores the similarity of user2, user3, user4, and user5 to user6\n",
    "knn_sims = np.array([0.8, 0.92, 0.75, 0.83])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also their rating on the `Machine Learning Capstone` course are:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.0 means audit and 3.0 means complete the course\n",
    "knn_ratings = np.array([3.0, 3.0, 2.0, 3.0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the predicted rating of `user6` to item `Machine Learning Capstone` course can be calculated as:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7727272727272725"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_u6_ml =  np.dot(knn_sims, knn_ratings)/ sum(knn_sims)\n",
    "r_u6_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we already know the true rating to be 3.0, then we get a prediction error RMSE (Rooted Mean Squared Error) as:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22727272727272751"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_rating = 3.0\n",
    "rmse = math.sqrt(true_rating - r_u6_ml) ** 2\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted rating is around 2.7 (close to 3.0 with RMSE 0.22), which indicates that `user6` is also likely to complete the course `Machine Learning Capstone`. As such, we may recommend it to user6 with high confidence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing this lab you will be able to:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Perform KNN-based collaborative filtering on the user-item interaction matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and exploring dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first load our dataset, i.e., a user-item (learn-course) interaction matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-ML0321EN-Coursera/labs/v2/module_3/ratings.csv\"\n",
    "rating_df = pd.read_csv(rating_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1889878</td>\n",
       "      <td>CC0101EN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1342067</td>\n",
       "      <td>CL0101EN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1990814</td>\n",
       "      <td>ML0120ENv3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>380098</td>\n",
       "      <td>BD0211EN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>779563</td>\n",
       "      <td>DS0101EN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user        item  rating\n",
       "0  1889878    CC0101EN       5\n",
       "1  1342067    CL0101EN       3\n",
       "2  1990814  ML0120ENv3       5\n",
       "3   380098    BD0211EN       5\n",
       "4   779563    DS0101EN       3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains three columns, `user id` (learner), `item id`(course), and `rating`(enrollment mode). \n",
    "\n",
    "Note that this matrix is presented as the dense or vertical form, and you may convert it to a sparse matrix using `pivot` :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>AI0111EN</th>\n",
       "      <th>BC0101EN</th>\n",
       "      <th>BC0201EN</th>\n",
       "      <th>BC0202EN</th>\n",
       "      <th>BD0101EN</th>\n",
       "      <th>BD0111EN</th>\n",
       "      <th>BD0115EN</th>\n",
       "      <th>BD0121EN</th>\n",
       "      <th>BD0123EN</th>\n",
       "      <th>...</th>\n",
       "      <th>SW0201EN</th>\n",
       "      <th>TA0105</th>\n",
       "      <th>TA0105EN</th>\n",
       "      <th>TA0106EN</th>\n",
       "      <th>TMP0101EN</th>\n",
       "      <th>TMP0105EN</th>\n",
       "      <th>TMP0106</th>\n",
       "      <th>TMP107</th>\n",
       "      <th>WA0101EN</th>\n",
       "      <th>WA0103EN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 127 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  AI0111EN  BC0101EN  BC0201EN  BC0202EN  BD0101EN  BD0111EN  BD0115EN  \\\n",
       "0     2       0.0       4.0       0.0       0.0       5.0       4.0       0.0   \n",
       "1     4       0.0       0.0       0.0       0.0       5.0       3.0       4.0   \n",
       "2     5       3.0       5.0       5.0       0.0       4.0       0.0       0.0   \n",
       "3     7       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "4     8       0.0       0.0       0.0       0.0       0.0       3.0       0.0   \n",
       "\n",
       "   BD0121EN  BD0123EN  ...  SW0201EN  TA0105  TA0105EN  TA0106EN  TMP0101EN  \\\n",
       "0       5.0       3.0  ...       0.0     5.0       0.0       4.0        0.0   \n",
       "1       5.0       3.0  ...       0.0     4.0       0.0       0.0        0.0   \n",
       "2       0.0       3.0  ...       0.0     0.0       4.0       4.0        4.0   \n",
       "3       0.0       0.0  ...       0.0     0.0       0.0       0.0        0.0   \n",
       "4       0.0       0.0  ...       0.0     0.0       0.0       0.0        0.0   \n",
       "\n",
       "   TMP0105EN  TMP0106  TMP107  WA0101EN  WA0103EN  \n",
       "0        3.0      3.0     0.0       5.0       0.0  \n",
       "1        3.0      3.0     0.0       3.0       3.0  \n",
       "2        4.0      4.0     5.0       0.0       3.0  \n",
       "3        0.0      0.0     0.0       0.0       0.0  \n",
       "4        0.0      0.0     0.0       0.0       0.0  \n",
       "\n",
       "[5 rows x 127 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_sparse_df = rating_df.pivot(index='user', columns='item', values='rating').fillna(0).reset_index().rename_axis(index=None, columns=None)\n",
    "rating_sparse_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, the dense format is more preferred as it saves a lot of storage and memory space. While the benefit of the sparse matrix is it is in the nature matrix format and you could apply computations such as cosine similarity directly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to perform KNN-based collaborative filtering on the user-item interaction matrix. \n",
    "You may choose one of the two following implementation options of KNN-based collaborative filtering. \n",
    "- The first one is to use `scikit-surprise` which is a popular and easy-to-use Python recommendation system library. \n",
    "- The second way is to implement it with standard `numpy`, `pandas`, and `sklearn`. You may need to write a lot of low-level implementation code along the way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Option 1: Use **Surprise** library (recommended)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Surprise* is a Python sci-kit library for recommender systems. It is simple and comprehensive to build and test different recommendation algorithms. \n",
    "\n",
    "First, let's install it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-surprise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import required classes and methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import KNNBasic\n",
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's take a look at a code example how easily to perform KNN collaborative filtering on a sample movie review dataset, which contains about 100k movie ratings from users.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to download dataset from https://files.grouplens.org/datasets/movielens/ml-100k.zip...\n",
      "Done! Dataset ml-100k has been saved to C:\\Users\\safaa/.surprise_data/ml-100k\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.9735\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9735301890502343"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the movielens-100k dataset (download it if needed),\n",
    "data = Dataset.load_builtin('ml-100k', prompt=False)\n",
    "\n",
    "# sample random trainset and testset\n",
    "# test set is made of 25% of the ratings.\n",
    "trainset, testset = train_test_split(data, test_size=.25)\n",
    "\n",
    "# We'll use the famous KNNBasic algorithm.\n",
    "algo = KNNBasic()\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Then compute RMSE\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, just a couple of lines and you can apply KNN collaborative filtering on the sample movie lens dataset. The main evaluation metric is `Root Mean Square Error (RMSE)` which is a very popular rating estimation error metric used in recommender systems as well as many regression model evaluations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load our own course rating dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the rating dataframe to a CSV file\n",
    "rating_df.to_csv(\"course_ratings.csv\", index=False)\n",
    "\n",
    "# Read the course rating dataset with columns user item rating\n",
    "reader = Reader(\n",
    "    line_format='user item rating', sep=',', skip_lines=1, rating_scale=(2, 3))\n",
    "\n",
    "# Load the dataset from the CSV file\n",
    "course_dataset = Dataset.load_from_file(\"course_ratings.csv\", reader=reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split it into trainset and testset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(course_dataset, test_size=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then check how many users and items we can use to fit a KNN model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 31328 users and 122 items in the trainingset\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total {trainset.n_users} users and {trainset.n_items} items in the trainingset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK: Perform KNN-based collaborative filtering on the user-item interaction matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_TODO: Fit the KNN-based collaborative filtering model using the trainset and evaluate the results using the testset:_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WRITE YOUR CODE HERE:\n",
    "\n",
    "\n",
    "# - Define a KNNBasic() model\n",
    "# Note there are some arguments such as:\n",
    "# max_k and min_k, representing the max and min number of neighors for rating estimations\n",
    "# sim_option, representing similarity measurement such as cosine and whether you want it to be user_based or items_based \n",
    "# e.g., sim_option = {\n",
    "#        'name': 'cosine', 'user_based': False,\n",
    "#    }\n",
    "#\n",
    "# more KNN model hyperparamets can be found here:\n",
    "# https://surprise.readthedocs.io/en/stable/knn_inspired.html\n",
    "# \n",
    "# You may try different hyperparamet combinations to see which one has the best performance\n",
    "\n",
    "\n",
    "# - Train the KNNBasic model on the trainset, and predict ratings for the testset\n",
    "\n",
    "# - Then compute RMSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.2876\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.287579472649509"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define similarity options dictionary\n",
    "sim_option = {'name': 'cosine', 'user_based': True}\n",
    "\n",
    "# Define a KNNBasic() model\n",
    "model_s = KNNBasic()\n",
    "\n",
    "# Train the KNNBasic model on the trainset\n",
    "model_s.fit(trainset)\n",
    "\n",
    "# predict ratings for the testset\n",
    "predictions_s = model_s.test(testset)\n",
    "\n",
    "# compute RMSE\n",
    "rmse_s = accuracy.rmse(predictions_s)\n",
    "\n",
    "rmse_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Hints</summary>\n",
    "\n",
    "* Create a model by calling `KNNBasic()` class. \n",
    "* Fit it with `trainset` by using `model.fit(trainset)`.  \n",
    "* Record predictions to the `testset`  by using `model.test(testset).\n",
    "* Compute the accuracy by using `accuracy.rmse(predictions)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn more detailed usages about _Surprise_ library, visit its website from [here](https://surprise.readthedocs.io/en/stable/getting_started.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML321ENSkillsNetwork817-2022-01-01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Option 2: Use `numpy`, `pandas`, and `sklearn`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not prefer the one-stop Suprise solution and want more hardcore coding practices, you may implement the KNN model using `numpy`, `pandas`, and possibly `sklearn`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create course id to index and index to id mappings\n",
    "def get_users_dicts(rating_df):\n",
    "    # Group the DataFrame by course index and ID, and get the maximum value for each group\n",
    "    grouped_df = rating_df.groupby(['user']).max().reset_index(drop=False)\n",
    "    # Create a dictionary mapping indices to course IDs\n",
    "    idx_id_dict = grouped_df[['user']].to_dict()['user']\n",
    "    # Create a dictionary mapping course IDs to indices\n",
    "    id_idx_dict = {v: k for k, v in idx_id_dict.items()}\n",
    "    # Clean up temporary DataFrame\n",
    "    del grouped_df\n",
    "    return idx_id_dict, id_idx_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 2,\n",
       " 1: 4,\n",
       " 2: 5,\n",
       " 3: 7,\n",
       " 4: 8,\n",
       " 5: 9,\n",
       " 6: 12,\n",
       " 7: 16,\n",
       " 8: 17,\n",
       " 9: 19,\n",
       " 10: 20,\n",
       " 11: 21,\n",
       " 12: 22,\n",
       " 13: 23,\n",
       " 14: 25,\n",
       " 15: 26,\n",
       " 16: 27,\n",
       " 17: 28,\n",
       " 18: 29,\n",
       " 19: 30,\n",
       " 20: 33,\n",
       " 21: 34,\n",
       " 22: 35,\n",
       " 23: 36,\n",
       " 24: 38,\n",
       " 25: 39,\n",
       " 26: 40,\n",
       " 27: 41,\n",
       " 28: 42,\n",
       " 29: 43,\n",
       " 30: 44,\n",
       " 31: 45,\n",
       " 32: 46,\n",
       " 33: 47,\n",
       " 34: 48,\n",
       " 35: 49,\n",
       " 36: 51,\n",
       " 37: 52,\n",
       " 38: 53,\n",
       " 39: 54,\n",
       " 40: 55,\n",
       " 41: 56,\n",
       " 42: 57,\n",
       " 43: 58,\n",
       " 44: 59,\n",
       " 45: 60,\n",
       " 46: 61,\n",
       " 47: 62,\n",
       " 48: 63,\n",
       " 49: 64,\n",
       " 50: 65,\n",
       " 51: 66,\n",
       " 52: 67,\n",
       " 53: 68,\n",
       " 54: 78,\n",
       " 55: 79,\n",
       " 56: 80,\n",
       " 57: 93,\n",
       " 58: 102,\n",
       " 59: 103,\n",
       " 60: 104,\n",
       " 61: 106,\n",
       " 62: 107,\n",
       " 63: 108,\n",
       " 64: 109,\n",
       " 65: 110,\n",
       " 66: 111,\n",
       " 67: 112,\n",
       " 68: 113,\n",
       " 69: 114,\n",
       " 70: 116,\n",
       " 71: 117,\n",
       " 72: 118,\n",
       " 73: 119,\n",
       " 74: 120,\n",
       " 75: 121,\n",
       " 76: 122,\n",
       " 77: 123,\n",
       " 78: 125,\n",
       " 79: 131,\n",
       " 80: 132,\n",
       " 81: 133,\n",
       " 82: 134,\n",
       " 83: 135,\n",
       " 84: 136,\n",
       " 85: 137,\n",
       " 86: 138,\n",
       " 87: 146,\n",
       " 88: 147,\n",
       " 89: 154,\n",
       " 90: 452,\n",
       " 91: 589,\n",
       " 92: 704,\n",
       " 93: 768,\n",
       " 94: 866,\n",
       " 95: 1321,\n",
       " 96: 1383,\n",
       " 97: 1804,\n",
       " 98: 1938,\n",
       " 99: 2191,\n",
       " 100: 2674,\n",
       " 101: 3087,\n",
       " 102: 3264,\n",
       " 103: 3450,\n",
       " 104: 4930,\n",
       " 105: 5053,\n",
       " 106: 5555,\n",
       " 107: 6002,\n",
       " 108: 6092,\n",
       " 109: 6449,\n",
       " 110: 6847,\n",
       " 111: 6868,\n",
       " 112: 7233,\n",
       " 113: 7359,\n",
       " 114: 7420,\n",
       " 115: 7438,\n",
       " 116: 7442,\n",
       " 117: 7458,\n",
       " 118: 7575,\n",
       " 119: 7702,\n",
       " 120: 7764,\n",
       " 121: 7789,\n",
       " 122: 7909,\n",
       " 123: 7928,\n",
       " 124: 8030,\n",
       " 125: 8059,\n",
       " 126: 8075,\n",
       " 127: 8805,\n",
       " 128: 9954,\n",
       " 129: 10215,\n",
       " 130: 10694,\n",
       " 131: 11633,\n",
       " 132: 12273,\n",
       " 133: 12462,\n",
       " 134: 12833,\n",
       " 135: 12834,\n",
       " 136: 13224,\n",
       " 137: 13365,\n",
       " 138: 13944,\n",
       " 139: 14010,\n",
       " 140: 14030,\n",
       " 141: 14322,\n",
       " 142: 14392,\n",
       " 143: 14811,\n",
       " 144: 15055,\n",
       " 145: 15508,\n",
       " 146: 15577,\n",
       " 147: 15662,\n",
       " 148: 16030,\n",
       " 149: 16164,\n",
       " 150: 16941,\n",
       " 151: 16951,\n",
       " 152: 16963,\n",
       " 153: 16982,\n",
       " 154: 18040,\n",
       " 155: 18169,\n",
       " 156: 18508,\n",
       " 157: 19496,\n",
       " 158: 19661,\n",
       " 159: 20292,\n",
       " 160: 20833,\n",
       " 161: 20948,\n",
       " 162: 21475,\n",
       " 163: 21577,\n",
       " 164: 21709,\n",
       " 165: 21750,\n",
       " 166: 22492,\n",
       " 167: 22708,\n",
       " 168: 23641,\n",
       " 169: 23794,\n",
       " 170: 23877,\n",
       " 171: 24234,\n",
       " 172: 24632,\n",
       " 173: 24743,\n",
       " 174: 24868,\n",
       " 175: 25618,\n",
       " 176: 26339,\n",
       " 177: 26454,\n",
       " 178: 26609,\n",
       " 179: 28895,\n",
       " 180: 29733,\n",
       " 181: 29962,\n",
       " 182: 30031,\n",
       " 183: 30155,\n",
       " 184: 30281,\n",
       " 185: 30958,\n",
       " 186: 31000,\n",
       " 187: 31054,\n",
       " 188: 32009,\n",
       " 189: 32318,\n",
       " 190: 32331,\n",
       " 191: 32444,\n",
       " 192: 32658,\n",
       " 193: 32917,\n",
       " 194: 32936,\n",
       " 195: 32975,\n",
       " 196: 33218,\n",
       " 197: 33357,\n",
       " 198: 33474,\n",
       " 199: 33565,\n",
       " 200: 34038,\n",
       " 201: 34074,\n",
       " 202: 34572,\n",
       " 203: 35928,\n",
       " 204: 36235,\n",
       " 205: 36321,\n",
       " 206: 36485,\n",
       " 207: 36585,\n",
       " 208: 37444,\n",
       " 209: 37451,\n",
       " 210: 37465,\n",
       " 211: 37688,\n",
       " 212: 37892,\n",
       " 213: 37936,\n",
       " 214: 38699,\n",
       " 215: 38951,\n",
       " 216: 39340,\n",
       " 217: 39919,\n",
       " 218: 40303,\n",
       " 219: 40562,\n",
       " 220: 40661,\n",
       " 221: 41727,\n",
       " 222: 41881,\n",
       " 223: 42044,\n",
       " 224: 42266,\n",
       " 225: 42433,\n",
       " 226: 42528,\n",
       " 227: 43309,\n",
       " 228: 43341,\n",
       " 229: 43567,\n",
       " 230: 43704,\n",
       " 231: 44318,\n",
       " 232: 44500,\n",
       " 233: 44577,\n",
       " 234: 44857,\n",
       " 235: 45457,\n",
       " 236: 45929,\n",
       " 237: 46170,\n",
       " 238: 46272,\n",
       " 239: 46326,\n",
       " 240: 46628,\n",
       " 241: 47459,\n",
       " 242: 47460,\n",
       " 243: 47574,\n",
       " 244: 48871,\n",
       " 245: 49313,\n",
       " 246: 49319,\n",
       " 247: 49400,\n",
       " 248: 49508,\n",
       " 249: 50348,\n",
       " 250: 50494,\n",
       " 251: 50667,\n",
       " 252: 50831,\n",
       " 253: 51452,\n",
       " 254: 51937,\n",
       " 255: 52091,\n",
       " 256: 52262,\n",
       " 257: 52917,\n",
       " 258: 53270,\n",
       " 259: 53345,\n",
       " 260: 54439,\n",
       " 261: 55117,\n",
       " 262: 55937,\n",
       " 263: 55958,\n",
       " 264: 56047,\n",
       " 265: 56882,\n",
       " 266: 56991,\n",
       " 267: 57052,\n",
       " 268: 58646,\n",
       " 269: 58895,\n",
       " 270: 58969,\n",
       " 271: 59042,\n",
       " 272: 59106,\n",
       " 273: 59486,\n",
       " 274: 59791,\n",
       " 275: 60054,\n",
       " 276: 60465,\n",
       " 277: 61658,\n",
       " 278: 61664,\n",
       " 279: 61747,\n",
       " 280: 62816,\n",
       " 281: 63000,\n",
       " 282: 63535,\n",
       " 283: 64056,\n",
       " 284: 64209,\n",
       " 285: 64321,\n",
       " 286: 64634,\n",
       " 287: 64669,\n",
       " 288: 64753,\n",
       " 289: 65093,\n",
       " 290: 65118,\n",
       " 291: 65307,\n",
       " 292: 65475,\n",
       " 293: 65761,\n",
       " 294: 66190,\n",
       " 295: 66648,\n",
       " 296: 66779,\n",
       " 297: 67196,\n",
       " 298: 67201,\n",
       " 299: 67305,\n",
       " 300: 68209,\n",
       " 301: 69124,\n",
       " 302: 69129,\n",
       " 303: 69649,\n",
       " 304: 69687,\n",
       " 305: 69955,\n",
       " 306: 70221,\n",
       " 307: 70434,\n",
       " 308: 70452,\n",
       " 309: 70612,\n",
       " 310: 70733,\n",
       " 311: 70934,\n",
       " 312: 72417,\n",
       " 313: 72482,\n",
       " 314: 72586,\n",
       " 315: 72632,\n",
       " 316: 72806,\n",
       " 317: 73271,\n",
       " 318: 73328,\n",
       " 319: 73345,\n",
       " 320: 73425,\n",
       " 321: 73660,\n",
       " 322: 73922,\n",
       " 323: 74781,\n",
       " 324: 74799,\n",
       " 325: 74945,\n",
       " 326: 74982,\n",
       " 327: 75103,\n",
       " 328: 75228,\n",
       " 329: 75362,\n",
       " 330: 75368,\n",
       " 331: 75692,\n",
       " 332: 76145,\n",
       " 333: 76232,\n",
       " 334: 76242,\n",
       " 335: 76244,\n",
       " 336: 76391,\n",
       " 337: 76432,\n",
       " 338: 76858,\n",
       " 339: 76944,\n",
       " 340: 77114,\n",
       " 341: 77247,\n",
       " 342: 77325,\n",
       " 343: 77474,\n",
       " 344: 77479,\n",
       " 345: 77747,\n",
       " 346: 78170,\n",
       " 347: 78644,\n",
       " 348: 79064,\n",
       " 349: 79582,\n",
       " 350: 79620,\n",
       " 351: 79633,\n",
       " 352: 79759,\n",
       " 353: 80010,\n",
       " 354: 80186,\n",
       " 355: 80219,\n",
       " 356: 80360,\n",
       " 357: 80596,\n",
       " 358: 81341,\n",
       " 359: 81700,\n",
       " 360: 81910,\n",
       " 361: 82579,\n",
       " 362: 82719,\n",
       " 363: 82787,\n",
       " 364: 82804,\n",
       " 365: 82963,\n",
       " 366: 83378,\n",
       " 367: 83447,\n",
       " 368: 83795,\n",
       " 369: 83817,\n",
       " 370: 84146,\n",
       " 371: 84148,\n",
       " 372: 84430,\n",
       " 373: 84433,\n",
       " 374: 85085,\n",
       " 375: 85625,\n",
       " 376: 85746,\n",
       " 377: 85940,\n",
       " 378: 86171,\n",
       " 379: 86305,\n",
       " 380: 86366,\n",
       " 381: 86548,\n",
       " 382: 86587,\n",
       " 383: 86648,\n",
       " 384: 86996,\n",
       " 385: 87460,\n",
       " 386: 87486,\n",
       " 387: 87533,\n",
       " 388: 87969,\n",
       " 389: 88085,\n",
       " 390: 88121,\n",
       " 391: 88202,\n",
       " 392: 88273,\n",
       " 393: 88421,\n",
       " 394: 90144,\n",
       " 395: 90850,\n",
       " 396: 90934,\n",
       " 397: 91071,\n",
       " 398: 91117,\n",
       " 399: 91242,\n",
       " 400: 91539,\n",
       " 401: 91852,\n",
       " 402: 92073,\n",
       " 403: 92472,\n",
       " 404: 92538,\n",
       " 405: 92806,\n",
       " 406: 92922,\n",
       " 407: 92990,\n",
       " 408: 93025,\n",
       " 409: 93228,\n",
       " 410: 93299,\n",
       " 411: 94135,\n",
       " 412: 94272,\n",
       " 413: 94604,\n",
       " 414: 94815,\n",
       " 415: 94853,\n",
       " 416: 95333,\n",
       " 417: 96035,\n",
       " 418: 96046,\n",
       " 419: 96961,\n",
       " 420: 96962,\n",
       " 421: 96995,\n",
       " 422: 97204,\n",
       " 423: 97254,\n",
       " 424: 97822,\n",
       " 425: 97915,\n",
       " 426: 98282,\n",
       " 427: 98471,\n",
       " 428: 98782,\n",
       " 429: 99234,\n",
       " 430: 99909,\n",
       " 431: 99954,\n",
       " 432: 100209,\n",
       " 433: 100274,\n",
       " 434: 100566,\n",
       " 435: 100638,\n",
       " 436: 100697,\n",
       " 437: 100942,\n",
       " 438: 101000,\n",
       " 439: 101367,\n",
       " 440: 101585,\n",
       " 441: 101628,\n",
       " 442: 101774,\n",
       " 443: 101802,\n",
       " 444: 102140,\n",
       " 445: 102360,\n",
       " 446: 102546,\n",
       " 447: 102568,\n",
       " 448: 102599,\n",
       " 449: 102689,\n",
       " 450: 102696,\n",
       " 451: 103036,\n",
       " 452: 103066,\n",
       " 453: 103234,\n",
       " 454: 103645,\n",
       " 455: 103968,\n",
       " 456: 104249,\n",
       " 457: 104394,\n",
       " 458: 105018,\n",
       " 459: 105163,\n",
       " 460: 105241,\n",
       " 461: 105467,\n",
       " 462: 105564,\n",
       " 463: 105794,\n",
       " 464: 106397,\n",
       " 465: 107179,\n",
       " 466: 107320,\n",
       " 467: 107634,\n",
       " 468: 107736,\n",
       " 469: 107882,\n",
       " 470: 108331,\n",
       " 471: 108541,\n",
       " 472: 108647,\n",
       " 473: 109351,\n",
       " 474: 109395,\n",
       " 475: 109445,\n",
       " 476: 109580,\n",
       " 477: 109673,\n",
       " 478: 109678,\n",
       " 479: 109711,\n",
       " 480: 109915,\n",
       " 481: 110030,\n",
       " 482: 110319,\n",
       " 483: 110519,\n",
       " 484: 110877,\n",
       " 485: 111049,\n",
       " 486: 112224,\n",
       " 487: 112225,\n",
       " 488: 113822,\n",
       " 489: 113910,\n",
       " 490: 114249,\n",
       " 491: 114398,\n",
       " 492: 115123,\n",
       " 493: 115500,\n",
       " 494: 115638,\n",
       " 495: 115921,\n",
       " 496: 116132,\n",
       " 497: 116174,\n",
       " 498: 116366,\n",
       " 499: 117569,\n",
       " 500: 117635,\n",
       " 501: 117789,\n",
       " 502: 117816,\n",
       " 503: 117831,\n",
       " 504: 117849,\n",
       " 505: 117876,\n",
       " 506: 118927,\n",
       " 507: 119014,\n",
       " 508: 119617,\n",
       " 509: 119644,\n",
       " 510: 119904,\n",
       " 511: 120240,\n",
       " 512: 120443,\n",
       " 513: 120622,\n",
       " 514: 120796,\n",
       " 515: 121300,\n",
       " 516: 121543,\n",
       " 517: 121657,\n",
       " 518: 121749,\n",
       " 519: 121751,\n",
       " 520: 121943,\n",
       " 521: 122122,\n",
       " 522: 122962,\n",
       " 523: 123033,\n",
       " 524: 123234,\n",
       " 525: 123343,\n",
       " 526: 123412,\n",
       " 527: 124060,\n",
       " 528: 124210,\n",
       " 529: 124271,\n",
       " 530: 124515,\n",
       " 531: 124606,\n",
       " 532: 124716,\n",
       " 533: 124729,\n",
       " 534: 124774,\n",
       " 535: 125262,\n",
       " 536: 125267,\n",
       " 537: 125440,\n",
       " 538: 125493,\n",
       " 539: 125668,\n",
       " 540: 126106,\n",
       " 541: 126613,\n",
       " 542: 126632,\n",
       " 543: 128463,\n",
       " 544: 128707,\n",
       " 545: 128853,\n",
       " 546: 129117,\n",
       " 547: 129269,\n",
       " 548: 129624,\n",
       " 549: 129727,\n",
       " 550: 129732,\n",
       " 551: 129965,\n",
       " 552: 130101,\n",
       " 553: 130213,\n",
       " 554: 130217,\n",
       " 555: 130256,\n",
       " 556: 130477,\n",
       " 557: 130563,\n",
       " 558: 130589,\n",
       " 559: 130787,\n",
       " 560: 130816,\n",
       " 561: 131799,\n",
       " 562: 132537,\n",
       " 563: 132543,\n",
       " 564: 132821,\n",
       " 565: 132827,\n",
       " 566: 133051,\n",
       " 567: 133246,\n",
       " 568: 133318,\n",
       " 569: 133441,\n",
       " 570: 133763,\n",
       " 571: 134409,\n",
       " 572: 134646,\n",
       " 573: 134697,\n",
       " 574: 135243,\n",
       " 575: 135937,\n",
       " 576: 135970,\n",
       " 577: 136152,\n",
       " 578: 136350,\n",
       " 579: 136666,\n",
       " 580: 136675,\n",
       " 581: 136944,\n",
       " 582: 137278,\n",
       " 583: 137381,\n",
       " 584: 137541,\n",
       " 585: 137629,\n",
       " 586: 137735,\n",
       " 587: 137901,\n",
       " 588: 138046,\n",
       " 589: 138087,\n",
       " 590: 138320,\n",
       " 591: 138452,\n",
       " 592: 138485,\n",
       " 593: 138528,\n",
       " 594: 138664,\n",
       " 595: 138759,\n",
       " 596: 138848,\n",
       " 597: 138903,\n",
       " 598: 138916,\n",
       " 599: 138939,\n",
       " 600: 139107,\n",
       " 601: 139211,\n",
       " 602: 139233,\n",
       " 603: 139505,\n",
       " 604: 139584,\n",
       " 605: 139739,\n",
       " 606: 139748,\n",
       " 607: 139818,\n",
       " 608: 139829,\n",
       " 609: 140424,\n",
       " 610: 140760,\n",
       " 611: 141000,\n",
       " 612: 141128,\n",
       " 613: 141193,\n",
       " 614: 141484,\n",
       " 615: 141624,\n",
       " 616: 142368,\n",
       " 617: 142391,\n",
       " 618: 142685,\n",
       " 619: 143623,\n",
       " 620: 143660,\n",
       " 621: 143871,\n",
       " 622: 144117,\n",
       " 623: 144438,\n",
       " 624: 144669,\n",
       " 625: 145000,\n",
       " 626: 145074,\n",
       " 627: 145102,\n",
       " 628: 145369,\n",
       " 629: 145641,\n",
       " 630: 145790,\n",
       " 631: 146229,\n",
       " 632: 147000,\n",
       " 633: 147164,\n",
       " 634: 147400,\n",
       " 635: 147623,\n",
       " 636: 147671,\n",
       " 637: 148055,\n",
       " 638: 148135,\n",
       " 639: 148487,\n",
       " 640: 148622,\n",
       " 641: 149373,\n",
       " 642: 149382,\n",
       " 643: 149690,\n",
       " 644: 150230,\n",
       " 645: 150373,\n",
       " 646: 150419,\n",
       " 647: 150426,\n",
       " 648: 151494,\n",
       " 649: 151502,\n",
       " 650: 151584,\n",
       " 651: 151817,\n",
       " 652: 151987,\n",
       " 653: 152320,\n",
       " 654: 152398,\n",
       " 655: 152432,\n",
       " 656: 152613,\n",
       " 657: 153173,\n",
       " 658: 153803,\n",
       " 659: 153883,\n",
       " 660: 153966,\n",
       " 661: 154139,\n",
       " 662: 154394,\n",
       " 663: 154453,\n",
       " 664: 154589,\n",
       " 665: 154610,\n",
       " 666: 154922,\n",
       " 667: 155199,\n",
       " 668: 155206,\n",
       " 669: 155490,\n",
       " 670: 155724,\n",
       " 671: 156350,\n",
       " 672: 156370,\n",
       " 673: 156434,\n",
       " 674: 156730,\n",
       " 675: 156848,\n",
       " 676: 156860,\n",
       " 677: 157085,\n",
       " 678: 157230,\n",
       " 679: 157420,\n",
       " 680: 157438,\n",
       " 681: 157573,\n",
       " 682: 157758,\n",
       " 683: 157813,\n",
       " 684: 158249,\n",
       " 685: 158336,\n",
       " 686: 158514,\n",
       " 687: 158838,\n",
       " 688: 158905,\n",
       " 689: 159040,\n",
       " 690: 159518,\n",
       " 691: 159737,\n",
       " 692: 160015,\n",
       " 693: 160156,\n",
       " 694: 160962,\n",
       " 695: 161018,\n",
       " 696: 161151,\n",
       " 697: 161420,\n",
       " 698: 161454,\n",
       " 699: 161471,\n",
       " 700: 161608,\n",
       " 701: 161841,\n",
       " 702: 161857,\n",
       " 703: 162078,\n",
       " 704: 162373,\n",
       " 705: 162428,\n",
       " 706: 162442,\n",
       " 707: 162455,\n",
       " 708: 163586,\n",
       " 709: 163602,\n",
       " 710: 163731,\n",
       " 711: 163739,\n",
       " 712: 163927,\n",
       " 713: 164042,\n",
       " 714: 164150,\n",
       " 715: 164300,\n",
       " 716: 164979,\n",
       " 717: 165124,\n",
       " 718: 165209,\n",
       " 719: 165324,\n",
       " 720: 165458,\n",
       " 721: 165807,\n",
       " 722: 165874,\n",
       " 723: 166059,\n",
       " 724: 166365,\n",
       " 725: 166398,\n",
       " 726: 166523,\n",
       " 727: 166570,\n",
       " 728: 166699,\n",
       " 729: 166937,\n",
       " 730: 167078,\n",
       " 731: 167626,\n",
       " 732: 168999,\n",
       " 733: 169142,\n",
       " 734: 169472,\n",
       " 735: 169637,\n",
       " 736: 169798,\n",
       " 737: 169987,\n",
       " 738: 170036,\n",
       " 739: 170327,\n",
       " 740: 170474,\n",
       " 741: 170740,\n",
       " 742: 171084,\n",
       " 743: 171298,\n",
       " 744: 172063,\n",
       " 745: 172084,\n",
       " 746: 172245,\n",
       " 747: 172326,\n",
       " 748: 172551,\n",
       " 749: 173127,\n",
       " 750: 173164,\n",
       " 751: 173257,\n",
       " 752: 173635,\n",
       " 753: 173982,\n",
       " 754: 174260,\n",
       " 755: 174315,\n",
       " 756: 174418,\n",
       " 757: 174573,\n",
       " 758: 174890,\n",
       " 759: 175100,\n",
       " 760: 175118,\n",
       " 761: 175261,\n",
       " 762: 175293,\n",
       " 763: 175362,\n",
       " 764: 175455,\n",
       " 765: 175646,\n",
       " 766: 175715,\n",
       " 767: 175893,\n",
       " 768: 175921,\n",
       " 769: 175928,\n",
       " 770: 175946,\n",
       " 771: 175987,\n",
       " 772: 176714,\n",
       " 773: 176742,\n",
       " 774: 176858,\n",
       " 775: 177215,\n",
       " 776: 177477,\n",
       " 777: 177647,\n",
       " 778: 177666,\n",
       " 779: 177687,\n",
       " 780: 177772,\n",
       " 781: 178210,\n",
       " 782: 178919,\n",
       " 783: 179023,\n",
       " 784: 179213,\n",
       " 785: 179272,\n",
       " 786: 179356,\n",
       " 787: 179755,\n",
       " 788: 179777,\n",
       " 789: 180038,\n",
       " 790: 180624,\n",
       " 791: 181002,\n",
       " 792: 181124,\n",
       " 793: 181324,\n",
       " 794: 181401,\n",
       " 795: 181700,\n",
       " 796: 182039,\n",
       " 797: 182289,\n",
       " 798: 182505,\n",
       " 799: 182509,\n",
       " 800: 182699,\n",
       " 801: 182752,\n",
       " 802: 182788,\n",
       " 803: 182808,\n",
       " 804: 182837,\n",
       " 805: 182847,\n",
       " 806: 183396,\n",
       " 807: 184092,\n",
       " 808: 184189,\n",
       " 809: 184259,\n",
       " 810: 184690,\n",
       " 811: 184880,\n",
       " 812: 185017,\n",
       " 813: 185072,\n",
       " 814: 185438,\n",
       " 815: 185857,\n",
       " 816: 186048,\n",
       " 817: 186106,\n",
       " 818: 186608,\n",
       " 819: 186756,\n",
       " 820: 186875,\n",
       " 821: 187017,\n",
       " 822: 187184,\n",
       " 823: 187262,\n",
       " 824: 187279,\n",
       " 825: 187516,\n",
       " 826: 187819,\n",
       " 827: 187955,\n",
       " 828: 188044,\n",
       " 829: 188144,\n",
       " 830: 188947,\n",
       " 831: 189131,\n",
       " 832: 189172,\n",
       " 833: 189175,\n",
       " 834: 189192,\n",
       " 835: 189256,\n",
       " 836: 189426,\n",
       " 837: 189455,\n",
       " 838: 189802,\n",
       " 839: 189913,\n",
       " 840: 190195,\n",
       " 841: 190432,\n",
       " 842: 190579,\n",
       " 843: 190671,\n",
       " 844: 190817,\n",
       " 845: 190930,\n",
       " 846: 191197,\n",
       " 847: 191645,\n",
       " 848: 191776,\n",
       " 849: 191887,\n",
       " 850: 191921,\n",
       " 851: 191933,\n",
       " 852: 192081,\n",
       " 853: 192315,\n",
       " 854: 192333,\n",
       " 855: 192997,\n",
       " 856: 193204,\n",
       " 857: 193310,\n",
       " 858: 193528,\n",
       " 859: 193611,\n",
       " 860: 193620,\n",
       " 861: 193652,\n",
       " 862: 194223,\n",
       " 863: 194480,\n",
       " 864: 194822,\n",
       " 865: 195188,\n",
       " 866: 195287,\n",
       " 867: 195602,\n",
       " 868: 195609,\n",
       " 869: 195613,\n",
       " 870: 195824,\n",
       " 871: 195892,\n",
       " 872: 195976,\n",
       " 873: 196058,\n",
       " 874: 196709,\n",
       " 875: 196746,\n",
       " 876: 196790,\n",
       " 877: 196806,\n",
       " 878: 197189,\n",
       " 879: 197206,\n",
       " 880: 197218,\n",
       " 881: 198074,\n",
       " 882: 198492,\n",
       " 883: 198689,\n",
       " 884: 198691,\n",
       " 885: 199016,\n",
       " 886: 199198,\n",
       " 887: 199357,\n",
       " 888: 199489,\n",
       " 889: 200180,\n",
       " 890: 200396,\n",
       " 891: 200762,\n",
       " 892: 201282,\n",
       " 893: 201371,\n",
       " 894: 201737,\n",
       " 895: 202244,\n",
       " 896: 202365,\n",
       " 897: 202693,\n",
       " 898: 202721,\n",
       " 899: 203388,\n",
       " 900: 203703,\n",
       " 901: 204047,\n",
       " 902: 204056,\n",
       " 903: 204095,\n",
       " 904: 204355,\n",
       " 905: 204451,\n",
       " 906: 204653,\n",
       " 907: 204657,\n",
       " 908: 204680,\n",
       " 909: 204745,\n",
       " 910: 205130,\n",
       " 911: 205132,\n",
       " 912: 205331,\n",
       " 913: 205538,\n",
       " 914: 206419,\n",
       " 915: 206831,\n",
       " 916: 206962,\n",
       " 917: 207048,\n",
       " 918: 207227,\n",
       " 919: 207443,\n",
       " 920: 207613,\n",
       " 921: 207767,\n",
       " 922: 208361,\n",
       " 923: 208700,\n",
       " 924: 209002,\n",
       " 925: 209257,\n",
       " 926: 209266,\n",
       " 927: 209558,\n",
       " 928: 209646,\n",
       " 929: 209738,\n",
       " 930: 209900,\n",
       " 931: 209929,\n",
       " 932: 210117,\n",
       " 933: 210149,\n",
       " 934: 210423,\n",
       " 935: 210581,\n",
       " 936: 210831,\n",
       " 937: 211093,\n",
       " 938: 211105,\n",
       " 939: 211118,\n",
       " 940: 211325,\n",
       " 941: 211330,\n",
       " 942: 211375,\n",
       " 943: 211420,\n",
       " 944: 211683,\n",
       " 945: 211995,\n",
       " 946: 212040,\n",
       " 947: 212424,\n",
       " 948: 212936,\n",
       " 949: 213427,\n",
       " 950: 213673,\n",
       " 951: 214645,\n",
       " 952: 215171,\n",
       " 953: 215363,\n",
       " 954: 215376,\n",
       " 955: 215571,\n",
       " 956: 215694,\n",
       " 957: 215982,\n",
       " 958: 216066,\n",
       " 959: 216253,\n",
       " 960: 216458,\n",
       " 961: 217612,\n",
       " 962: 217818,\n",
       " 963: 218584,\n",
       " 964: 218617,\n",
       " 965: 218774,\n",
       " 966: 218810,\n",
       " 967: 218927,\n",
       " 968: 218982,\n",
       " 969: 219007,\n",
       " 970: 219201,\n",
       " 971: 219396,\n",
       " 972: 219453,\n",
       " 973: 219503,\n",
       " 974: 219724,\n",
       " 975: 219733,\n",
       " 976: 219831,\n",
       " 977: 219886,\n",
       " 978: 219935,\n",
       " 979: 220065,\n",
       " 980: 220688,\n",
       " 981: 220851,\n",
       " 982: 221062,\n",
       " 983: 221140,\n",
       " 984: 221232,\n",
       " 985: 221401,\n",
       " 986: 221548,\n",
       " 987: 223477,\n",
       " 988: 223512,\n",
       " 989: 223526,\n",
       " 990: 223583,\n",
       " 991: 223676,\n",
       " 992: 223704,\n",
       " 993: 223836,\n",
       " 994: 224055,\n",
       " 995: 224206,\n",
       " 996: 224246,\n",
       " 997: 224331,\n",
       " 998: 224562,\n",
       " 999: 224714,\n",
       " ...}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_id_dict, id_idx_dict = get_users_dicts(rating_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate users similarity matrix\n",
    "\n",
    "user_idxs = idx_id_dict.keys()\n",
    "sim_matrix = np.empty(shape=(len(user_idxs),len(user_idxs)))\n",
    "\n",
    "for id1 in user_idxs:\n",
    "    user1 = rating_sparse_df.iloc[id1,1:]\n",
    "    for id2 in user_idxs:\n",
    "        user2 = rating_sparse_df.iloc[id2,1:]  \n",
    "        sim_matrix[id1,id2] = round(1 - cosine(user1.values, user2.values), 2)\n",
    "\n",
    "sim_matrix[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WRITE YOUR CODE HERE:\n",
    "\n",
    "## One solution could be:\n",
    "## - Calculate the similarity between two users using their rating history (the row vectors of interaction matrix)\n",
    "\n",
    "## - Build a similarity matrix for each pair of users with the training dataset\n",
    "\n",
    "## - For each user, find its k nearest neighbors in the sim matrix\n",
    "\n",
    "## - For each rating in the test dataset, estimate its rating using the KNN collaborative filtering equations shown before\n",
    "\n",
    "## - Calculate RMSE for the entire test dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('course_ratings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this lab, you have learned and implemented KNN-based collaborative filtering. It is probably the simplest but very effective and intuitive collaborative filtering algorithm. Since it is based on KNN, it inherits the main characteristics of KNN such as memory-intensive because you need to maintain a huge similarity matrix among users or items. In the future labs, we will learn other types of collaborative filtering which do not rely on such a huge similarity matrix to make rating predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Yan Luo](https://www.linkedin.com/in/yan-luo-96288783/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Contributors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```toggle## Change Log\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```toggle|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "```\n",
    "```toggle|-|-|-|-|\n",
    "```\n",
    "```toggle|2021-10-25|1.0|Yan|Created the initial version|\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "prev_pub_hash": "155ca510415f0c86629879a1a8087e0d13be058c4d6afd98f04ebd5a465566b6"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
